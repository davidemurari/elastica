{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzvFW1OnHklV"
      },
      "source": [
        "# Main part of the code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9q-Sn9mIciK"
      },
      "source": [
        "#### Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6J10IbrFs3OT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "955f9b81-8545-4c94-95dd-c9437d5b5a30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.3.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.12.0)\n",
            "Requirement already satisfied: cmaes>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (0.10.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.21)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "#Importing necessary libraries\n",
        "!pip install optuna\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.autograd.functional import jacobian as jac\n",
        "from torch.func import jacfwd, vmap\n",
        "from csv import writer\n",
        "import optuna #for hyperparameter search"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STfDbJumK3Xb",
        "outputId": "8a988e3f-6f67-4508-9f2f-5439fc4158c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/elastica_davide')"
      ],
      "metadata": {
        "id": "ADVl6_ayLk53"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scripts.createDataset import getData, getDataLoaders\n",
        "from scripts.utils import getBCs\n",
        "from scripts.network import approximate_curve\n",
        "from scripts.training import trainModel\n",
        "from scripts.evaluateModel import plotTestResults"
      ],
      "metadata": {
        "id": "D2_OcS7oKWO1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BZSovV7Wczb4"
      },
      "outputs": [],
      "source": [
        "#We do this so Pytorch works in double precision\n",
        "torch.set_default_dtype(torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJQy9f1dMtjT",
        "outputId": "7cba307b-e789-4506-8b0f-6c754572dffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7l2rASEIgtu"
      },
      "source": [
        "#### Importing and organising data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10W4hY82J9AX"
      },
      "source": [
        "We can create the dataset as follows:\n",
        "\n",
        "Input : (q_1,q_2,v_1,v_2,s)\n",
        "Output : (q(s),v(s))\n",
        "\n",
        "With the trajectories that we have available, we can hence generate a dataset of size\n",
        "N_elements+1 x N_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "X7AdGuZ2Mc8T"
      },
      "outputs": [],
      "source": [
        "#Importing and shuffling the trajectories\n",
        "\n",
        "trajectories = np.loadtxt(\"/content/drive/MyDrive/elastica_davide/data/generated_data.txt\")\n",
        "number_samples,number_components = trajectories.shape\n",
        "#Randomize the order of the trajectories\n",
        "indices = np.random.permutation(len(trajectories))\n",
        "trajectories = trajectories[indices]\n",
        "\n",
        "number_elements = int(number_components/4)-1\n",
        "data_train, data_test = getData(number_elements,number_samples,trajectories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZEskggrI0zk"
      },
      "source": [
        "#### Training the neural network iterating over the training batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZmhHT3cMULbB"
      },
      "outputs": [],
      "source": [
        "def define_model(trial):\n",
        "    normalize = trial.suggest_categorical(\"normalize\",[True,False])\n",
        "    netarch = trial.suggest_categorical(\"networkarch\",[0, 1, 2])\n",
        "    if netarch == 0:\n",
        "      is_deeponet = True\n",
        "      is_res = False\n",
        "    elif netarch == 1:\n",
        "      is_deeponet = False\n",
        "      is_res = True\n",
        "    else:\n",
        "      is_deeponet = False\n",
        "      is_res = False\n",
        "    act = trial.suggest_categorical(\"act\",['tanh','sigmoid','sin','swish'])\n",
        "    nlayers = trial.suggest_int(\"n_layers\", 1, 4)\n",
        "    hidden_nodes = trial.suggest_int(\"hidden_nodes\", 10, 100)\n",
        "    correct_functional = trial.suggest_categorical(\"correct_functional\",[True,False])\n",
        "\n",
        "    model = approximate_curve(normalize, act, nlayers, hidden_nodes, correct_functional, is_res, is_deeponet)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1QNrylGbURvx"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "\n",
        "    torch.manual_seed(1)\n",
        "    np.random.seed(1)\n",
        "\n",
        "    # Generate the model.\n",
        "    model = define_model(trial)\n",
        "    model.to(device);\n",
        "\n",
        "    lr = trial.suggest_float(\"lr\", 1e-4 , 1e-1, log=True)\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\",0,5e-4)\n",
        "    #optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
        "    #optimizer = getattr(torch.optim, optimizer_name)(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    #optimizer = torch.optim.LBFGS(model.parameters(),lr=lr, max_iter=100,tolerance_grad=1.e-10,\n",
        "    #                                  tolerance_change=1.e-10, history_size=100, line_search_fn='strong_wolfe')\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr=lr,weight_decay=weight_decay)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "\n",
        "    batch_size = 32 # trial.suggest_int(\"batch_size\", 32, 64)\n",
        "\n",
        "    trainloader, testloader = getDataLoaders(batch_size,data_train,data_test)\n",
        "\n",
        "    print(\"Current test with :\\n\\n\")\n",
        "    for key, value in trial.params.items():\n",
        "      print(\"    {}: {}\".format(key, value))\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "    epochs = 100\n",
        "    loss = trainModel(number_elements,device,model,criterion,optimizer,epochs,trainloader)\n",
        "    test_error = 100\n",
        "    if not torch.isnan(loss):\n",
        "      model.eval();\n",
        "\n",
        "      def eval_model(s,q1,q2,v1,v2):\n",
        "        s_ = torch.tensor([[s]],dtype=torch.float32).to(device)\n",
        "        q1 = torch.from_numpy(q1.astype(np.float32)).reshape(1,-1).to(device)\n",
        "        q2 = torch.from_numpy(q2.astype(np.float32)).reshape(1,-1).to(device)\n",
        "        v1 = torch.from_numpy(v1.astype(np.float32)).reshape(1,-1).to(device)\n",
        "        v2 = torch.from_numpy(v2.astype(np.float32)).reshape(1,-1).to(device)\n",
        "        return model(s_,q1,q2,v1,v2).detach().cpu().numpy()[0]\n",
        "\n",
        "      bcs = getBCs(trajectories)\n",
        "      q1 = bcs[\"q1\"]\n",
        "      q2 = bcs[\"q2\"]\n",
        "      v1 = bcs[\"v1\"]\n",
        "      v2 = bcs[\"v2\"]\n",
        "\n",
        "      xx = np.linspace(0, 1, number_elements+1)\n",
        "      res = np.zeros((50,2,len(xx)))\n",
        "\n",
        "      for j in range(50):\n",
        "          for i in range(len(xx)):\n",
        "              res[j,:,i] = eval_model(xx[i],q1[j],q2[j],v1[j],v2[j])\n",
        "\n",
        "      q_x_pred_torch = torch.from_numpy(res[:,0].astype(np.float32))\n",
        "      q_x_true_torch = torch.from_numpy(trajectories[:50,np.arange(0,number_components,4)].astype(np.float32))\n",
        "\n",
        "      q_y_pred_torch = torch.from_numpy(res[:,1].astype(np.float32))\n",
        "      q_y_true_torch = torch.from_numpy(trajectories[:50,np.arange(1,number_components,4)].astype(np.float32))\n",
        "\n",
        "      test_error = criterion(q_x_pred_torch,q_x_true_torch).item() + criterion(q_y_pred_torch,q_y_true_torch).item()\n",
        "\n",
        "    #Saving the obtained results\n",
        "    if trial.number == 0:\n",
        "        labels = []\n",
        "        for lab, _ in trial.params.items():\n",
        "            labels.append(str(lab))\n",
        "        labels.append(\"Test error\")\n",
        "        with open(\"/content/drive/MyDrive/elastica_davide/savedResults/results.csv\", \"a\") as f_object:\n",
        "            writer_object = writer(f_object)\n",
        "            writer_object.writerow(labels)\n",
        "            f_object.close()\n",
        "\n",
        "    results = []\n",
        "    for _, value in trial.params.items():\n",
        "        results.append(str(value))\n",
        "\n",
        "    results.append(test_error)\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/elastica_davide/savedResults/results.csv\", \"a\") as f_object:\n",
        "        writer_object = writer(f_object)\n",
        "        writer_object.writerow(results)\n",
        "        f_object.close()\n",
        "\n",
        "    return test_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZdWqmBOWfng",
        "outputId": "afd4ed3e-56f1-4953-d2a0-6477530cde65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Do you want to do hyperparameter test? Type Yes or No Yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-14 19:27:13,693] A new study created in memory with name: Euler Elastica\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current test with :\n",
            "\n",
            "\n",
            "    normalize: True\n",
            "    networkarch: 0\n",
            "    act: sigmoid\n",
            "    n_layers: 4\n",
            "    hidden_nodes: 96\n",
            "    correct_functional: False\n",
            "    lr: 0.0004001351626389534\n",
            "    weight_decay: 4.107793070450294e-05\n",
            "\n",
            "\n",
            "\n",
            "Loss [2](epoch):  0.3932371437549591\n",
            "Loss [3](epoch):  0.45897868275642395\n",
            "Loss [4](epoch):  0.4571979343891144\n",
            "Loss [5](epoch):  0.4476149082183838\n",
            "Loss [6](epoch):  0.36708560585975647\n",
            "Loss [7](epoch):  0.24484089016914368\n",
            "Loss [8](epoch):  0.21626968681812286\n",
            "Loss [9](epoch):  0.15656474232673645\n",
            "Loss [10](epoch):  0.12319495528936386\n",
            "Loss [11](epoch):  0.06442251801490784\n",
            "Loss [12](epoch):  0.04837831109762192\n",
            "Loss [13](epoch):  0.033051274716854095\n",
            "Loss [14](epoch):  0.025655072182416916\n",
            "Loss [15](epoch):  0.03498153015971184\n",
            "Loss [16](epoch):  0.02635006234049797\n",
            "Loss [17](epoch):  0.038770489394664764\n",
            "Loss [18](epoch):  0.027440451085567474\n",
            "Loss [19](epoch):  0.013566236943006516\n",
            "Loss [20](epoch):  0.022772496566176414\n",
            "Loss [21](epoch):  0.029205340892076492\n",
            "Loss [22](epoch):  0.017686549574136734\n",
            "Loss [23](epoch):  0.020790770649909973\n",
            "Loss [24](epoch):  0.029651496559381485\n",
            "Loss [25](epoch):  0.025747815147042274\n",
            "Loss [26](epoch):  0.02538842149078846\n",
            "Loss [27](epoch):  0.02922811731696129\n",
            "Loss [28](epoch):  0.021056856960058212\n",
            "Loss [29](epoch):  0.01810472644865513\n",
            "Loss [30](epoch):  0.04491522163152695\n",
            "Loss [31](epoch):  0.018325049430131912\n",
            "Loss [32](epoch):  0.02785174734890461\n",
            "Loss [33](epoch):  0.022469976916909218\n",
            "Loss [34](epoch):  0.019633078947663307\n",
            "Loss [35](epoch):  0.017222363501787186\n",
            "Loss [36](epoch):  0.020912686362862587\n",
            "Loss [37](epoch):  0.027902889996767044\n",
            "Loss [38](epoch):  0.02542552538216114\n",
            "Loss [39](epoch):  0.013965491205453873\n",
            "Loss [40](epoch):  0.017848633229732513\n",
            "Loss [41](epoch):  0.02080032229423523\n",
            "Loss [42](epoch):  0.009861444123089314\n",
            "Loss [43](epoch):  0.0211856197565794\n",
            "Loss [44](epoch):  0.022056298330426216\n",
            "Loss [45](epoch):  0.008837215602397919\n",
            "Loss [46](epoch):  0.03289967402815819\n",
            "Loss [47](epoch):  0.018817856907844543\n",
            "Loss [48](epoch):  0.025976058095693588\n",
            "Loss [49](epoch):  0.02164018712937832\n",
            "Loss [50](epoch):  0.012855798937380314\n",
            "Loss [51](epoch):  0.013965783640742302\n",
            "Loss [52](epoch):  0.02192910760641098\n",
            "Loss [53](epoch):  0.026372143998742104\n",
            "Loss [54](epoch):  0.012734334915876389\n",
            "Loss [55](epoch):  0.009570105001330376\n",
            "Loss [56](epoch):  0.01623668521642685\n",
            "Loss [57](epoch):  0.021688874810934067\n",
            "Loss [58](epoch):  0.01803472638130188\n",
            "Loss [59](epoch):  0.010619467124342918\n",
            "Loss [60](epoch):  0.016775257885456085\n",
            "Loss [61](epoch):  0.014069298282265663\n",
            "Loss [62](epoch):  0.017909301444888115\n",
            "Loss [63](epoch):  0.026036294177174568\n",
            "Loss [64](epoch):  0.03805408626794815\n",
            "Loss [65](epoch):  0.012742748484015465\n",
            "Loss [66](epoch):  0.012285706587135792\n",
            "Loss [67](epoch):  0.04603039473295212\n",
            "Loss [68](epoch):  0.009826850146055222\n",
            "Loss [69](epoch):  0.020899344235658646\n",
            "Loss [70](epoch):  0.016100604087114334\n",
            "Loss [71](epoch):  0.012893217615783215\n",
            "Loss [72](epoch):  0.03247695788741112\n",
            "Loss [73](epoch):  0.03596506267786026\n",
            "Loss [74](epoch):  0.022748032584786415\n",
            "Loss [75](epoch):  0.022649791091680527\n",
            "Loss [76](epoch):  0.01674683950841427\n",
            "Loss [77](epoch):  0.01200354378670454\n",
            "Loss [78](epoch):  0.01369252149015665\n",
            "Loss [79](epoch):  0.024240126833319664\n",
            "Loss [80](epoch):  0.012930741533637047\n",
            "Loss [81](epoch):  0.017402255907654762\n",
            "Loss [82](epoch):  0.01410193182528019\n",
            "Loss [83](epoch):  0.013063808903098106\n",
            "Loss [84](epoch):  0.017855655401945114\n",
            "Loss [85](epoch):  0.03446972370147705\n",
            "Loss [86](epoch):  0.017328746616840363\n",
            "Loss [87](epoch):  0.009739476256072521\n",
            "Loss [88](epoch):  0.009531274437904358\n",
            "Loss [89](epoch):  0.013465462252497673\n",
            "Loss [90](epoch):  0.020025452598929405\n",
            "Loss [91](epoch):  0.008331718854606152\n",
            "Loss [92](epoch):  0.022829731926321983\n",
            "Loss [93](epoch):  0.010908492840826511\n",
            "Loss [94](epoch):  0.04013645276427269\n",
            "Loss [95](epoch):  0.018731672316789627\n",
            "Loss [96](epoch):  0.016019029542803764\n",
            "Loss [97](epoch):  0.02661791816353798\n",
            "Loss [98](epoch):  0.03414754569530487\n",
            "Loss [99](epoch):  0.01564006321132183\n",
            "Loss [100](epoch):  0.026461679488420486\n",
            "Training Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2023-10-14 19:28:09,237] Trial 0 finished with value: 0.038577035011257976 and parameters: {'normalize': True, 'networkarch': 0, 'act': 'sigmoid', 'n_layers': 4, 'hidden_nodes': 96, 'correct_functional': False, 'lr': 0.0004001351626389534, 'weight_decay': 4.107793070450294e-05}. Best is trial 0 with value: 0.038577035011257976.\n"
          ]
        }
      ],
      "source": [
        "optuna_study = input(\"Do you want to do hyperparameter test? Type Yes or No \")\n",
        "params = {}\n",
        "if optuna_study==\"Yes\":\n",
        "    optuna_study = True\n",
        "else:\n",
        "    optuna_study = False\n",
        "if optuna_study:\n",
        "    study = optuna.create_study(direction=\"minimize\",study_name=\"Euler Elastica\")\n",
        "    study.optimize(objective, n_trials=100)\n",
        "    print(\"Study statistics: \")\n",
        "    print(\"  Number of finished trials: \", len(study.trials))\n",
        "\n",
        "    params = study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUXqJD8nJ9Af"
      },
      "source": [
        "#### Plot the results for the best combination of hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG72GxBPJ9Af"
      },
      "outputs": [],
      "source": [
        "if params=={}:\n",
        "    print(\"No parameters have been specified. Let's input them:\\n\\n\")\n",
        "    normalize = input(\"Normalize is True or False? \")==\"True\"\n",
        "    is_deeponet = input(\"Is DeepONet True or False? \")==\"True\"\n",
        "    if is_deeponet:\n",
        "      is_res = False\n",
        "    else:\n",
        "      is_res = input(\"Is_res True or False? \")==\"True\"\n",
        "    act = input(\"What activation function to use? Choose among 'sin', 'sigmoid', 'swish', 'tanh'\")\n",
        "    nlayers = int(input(\"How many layers do you want the network to have? \"))\n",
        "    hidden_nodes = int(input(\"How many hidden nodes do you want the network to have? \"))\n",
        "    correct_functional = input(\"Type True if you want to impose by design the BCs, False otherwise: \")\n",
        "    lr = float(input(\"What learning rate do you want to use? \"))\n",
        "    weight_decay = float(input(\"What weight decay do you want to use? \"))\n",
        "\n",
        "    params = {\"normalize\": normalize,\n",
        "              \"act\": act,\n",
        "              \"n_layers\":nlayers,\n",
        "              \"hidden_nodes\":hidden_nodes,\n",
        "              \"correct_functional\":correct_functional,\n",
        "              \"lr\":lr,\n",
        "              \"weight_decay\":weight_decay,\n",
        "              \"is_res\":is_res,\n",
        "              \"is_deeponet\":is_deeponet}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwcBoE2dJ9Ag"
      },
      "outputs": [],
      "source": [
        "def define_best_model():\n",
        "    normalize = params[\"normalize\"]\n",
        "    act = params[\"act\"]\n",
        "    nlayers = params[\"n_layers\"]\n",
        "    hidden_nodes = params[\"hidden_nodes\"]\n",
        "    correct_functional = params[\"correct_functional\"]\n",
        "    is_res = params[\"is_res\"]\n",
        "    is_deeponet = params[\"is_deeponet\"]\n",
        "\n",
        "    model = approximate_curve(normalize, act, nlayers, hidden_nodes, correct_functional, is_res, is_deeponet)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iimUr0a1J9Ag"
      },
      "outputs": [],
      "source": [
        "model = define_best_model()\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fc13QozJ9Ah"
      },
      "outputs": [],
      "source": [
        "weight_decay = params[\"weight_decay\"]\n",
        "lr = params[\"lr\"]\n",
        "#optimizer = torch.optim.LBFGS(model.parameters(),lr=lr, max_iter=100,tolerance_grad=1.e-10,\n",
        "#                                  tolerance_change=1.e-10, history_size=100, line_search_fn='strong_wolfe')\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=lr,weight_decay=weight_decay)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "trainloader, testloader = getDataLoaders(batch_size,data_train,data_test)\n",
        "epochs = 100\n",
        "loss = trainModel(number_elements,device,model,criterion,optimizer,epochs,trainloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmaNjZwKJ9Ai"
      },
      "outputs": [],
      "source": [
        "model.eval();\n",
        "plotTestResults(model,device,number_elements,number_components,trajectories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hKeSK8_J9Aj"
      },
      "outputs": [],
      "source": [
        "def eval_model(model,device,s,q1,q2,v1,v2):\n",
        "    s_ = torch.tensor([[s]],dtype=torch.float32).to(device)\n",
        "    q1 = torch.from_numpy(q1.astype(np.float32)).reshape(1,-1).to(device)\n",
        "    q2 = torch.from_numpy(q2.astype(np.float32)).reshape(1,-1).to(device)\n",
        "    v1 = torch.from_numpy(v1.astype(np.float32)).reshape(1,-1).to(device)\n",
        "    v2 = torch.from_numpy(v2.astype(np.float32)).reshape(1,-1).to(device)\n",
        "    return model(s_,q1,q2,v1,v2).detach().cpu().numpy()[0]\n",
        "\n",
        "#This returns the approximation vector q'(s) in R^2 associated to the boundary conditions (q1,q2,v1,v2)\n",
        "#and correspondent to position s in the interval [0,1]\n",
        "# TODO : Understand why this is not providing a good fit of the derivative\n",
        "def eval_derivative_model(model,device,s,q1,q2,v1,v2):\n",
        "    s_ = torch.tensor([[s]],dtype=torch.float32).to(device)\n",
        "    q1 = torch.from_numpy(q1.astype(np.float32)).reshape(1,-1).to(device)\n",
        "    q2 = torch.from_numpy(q2.astype(np.float32)).reshape(1,-1).to(device)\n",
        "    v1 = torch.from_numpy(v1.astype(np.float32)).reshape(1,-1).to(device)\n",
        "    v2 = torch.from_numpy(v2.astype(np.float32)).reshape(1,-1).to(device)\n",
        "\n",
        "    q = lambda s : model(s,q1,q2,v1,v2)[0]\n",
        "    v = lambda s : (jacfwd(q))(s)[:,:,0].T\n",
        "    return v(s_).detach().cpu().numpy().reshape(-1)\n",
        "\n",
        "def eval_derivative_model(model,device,s,q1,q2,v1,v2):\n",
        "    s_ = torch.tensor([[s]],dtype=torch.float32).to(device)\n",
        "    q1 = torch.from_numpy(q1.astype(np.float32)).reshape(1,-1).to(device)\n",
        "    q2 = torch.from_numpy(q2.astype(np.float32)).reshape(1,-1).to(device)\n",
        "    v1 = torch.from_numpy(v1.astype(np.float32)).reshape(1,-1).to(device)\n",
        "    v2 = torch.from_numpy(v2.astype(np.float32)).reshape(1,-1).to(device)\n",
        "\n",
        "    q = lambda s : model(s,q1,q2,v1,v2)[0]\n",
        "    v = lambda s : (jacfwd(q))(s)[:,:,0].T\n",
        "    return v(s_).detach().cpu().numpy().reshape(-1)\n",
        "\n",
        "def plotTestResults(model,device,number_elements,number_components,trajectories):\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    bcs = getBCs(trajectories)\n",
        "    q1 = bcs[\"q1\"]\n",
        "    q2 = bcs[\"q2\"]\n",
        "    v1 = bcs[\"v1\"]\n",
        "    v2 = bcs[\"v2\"]\n",
        "\n",
        "    xx = np.linspace(0, 1, number_elements+1)\n",
        "    res = np.zeros((len(trajectories),2,len(xx)))\n",
        "    res_derivative = np.zeros_like(res)\n",
        "\n",
        "    for j in range(20):\n",
        "        for i in range(len(xx)):\n",
        "            res[j,:,i] = eval_model(model,device,xx[i],q1[j],q2[j],v1[j],v2[j])\n",
        "            res_derivative[j,:,i] = eval_derivative_model(model,device,xx[i],q1[j],q2[j],v1[j],v2[j])\n",
        "\n",
        "    print(res[0])\n",
        "    print(res_derivative[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plotTestResults(model,device,number_elements,number_components,trajectories)"
      ],
      "metadata": {
        "id": "wILe7__wjBTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(trajectories[0])"
      ],
      "metadata": {
        "id": "nm6JF9PejHd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xVHFS5cCjlDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tXk3oONW3XSV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "ed7ab7e2e8a9240e70ddeb3e0a5d2646d3f0c1850e1b729c718218fffe2c99a3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}